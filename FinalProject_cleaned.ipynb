{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from sklearn.datasets import make_blobs\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "matchup_df = pd.read_csv('Data_Needs_updated_after_selection_sunday/Tournament Matchups.csv')\n",
        "all_time_results_df = pd.read_csv('Data_Needs_updated_after_selection_sunday/Team Results.csv') \n",
        "\n",
        "# Read both CSVs and set 'TEAM NO' as the index\n",
        "df_all = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/Resumes.csv\")\n",
        "df_temp = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/Shooting Splits.csv\")\n",
        "df_temp2 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/TeamRankings Away.csv\")\n",
        "df_temp3 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/TeamRankings Home.csv\")\n",
        "df_temp4 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/TeamRankings Neutral.csv\")\n",
        "df_temp5 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/KenPom Barttorvik.csv\")\n",
        "\n",
        "df_temp6 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/Barttorvik Away.csv\")\n",
        "df_temp7 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/Barttorvik Home.csv\")\n",
        "df_temp8 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/Barttorvik Neutral.csv\")\n",
        "#df_temp9 = pd.read_csv(\"./Data_Needs_updated_after_selection_sunday/EvanMiya.csv\")\n",
        "\n",
        "# removing columns that are not needed\n",
        "df_all = df_temp.drop(columns=['TEAM'])\n",
        "df_temp = df_temp.drop(columns=['YEAR', 'TEAM ID', 'TEAM', \"CONF\"])\n",
        "df_temp2 = df_temp2.drop(columns=['YEAR', 'SEED', 'ROUND', \"TEAM\"])\n",
        "df_temp3 = df_temp3.drop(columns=['YEAR', 'SEED', 'ROUND', \"TEAM\"])\n",
        "df_temp4 = df_temp4.drop(columns=['YEAR', 'SEED', 'ROUND', \"TEAM\"])\n",
        "df_temp5 = df_temp5.drop(columns=[\"YEAR\", \"SEED\", \"ROUND\", \"TEAM\", \"TEAM ID\", \"CONF\",\"CONF ID\",\"QUAD NO\",\"QUAD ID\"])\n",
        "\n",
        "df_temp6 = df_temp6.drop(columns=['YEAR', 'SEED', 'ROUND', \"TEAM\", \"TEAM ID\"])\n",
        "df_temp7 = df_temp7.drop(columns=['YEAR', 'SEED', 'ROUND', \"TEAM\", \"TEAM ID\"])\n",
        "df_temp8 = df_temp8.drop(columns=['YEAR', 'SEED', 'ROUND', \"TEAM ID\"])\n",
        "#df_temp9 = df_temp9.drop(columns=['YEAR', 'SEED', 'TEAM', 'ROUND'])\n",
        "\n",
        "# Standardize 'TEAM NO'\n",
        "df_all['TEAM NO'] = df_all['TEAM NO'].astype(str).str.strip()\n",
        "df_temp['TEAM NO'] = df_temp['TEAM NO'].astype(str).str.strip()\n",
        "df_temp2['TEAM NO'] = df_temp2['TEAM NO'].astype(str).str.strip()\n",
        "df_temp3['TEAM NO'] = df_temp3['TEAM NO'].astype(str).str.strip()\n",
        "df_temp4['TEAM NO'] = df_temp4['TEAM NO'].astype(str).str.strip()\n",
        "df_temp5['TEAM NO'] = df_temp5['TEAM NO'].astype(str).str.strip()\n",
        "\n",
        "df_temp6['TEAM NO'] = df_temp6['TEAM NO'].astype(str).str.strip()\n",
        "df_temp7['TEAM NO'] = df_temp7['TEAM NO'].astype(str).str.strip()\n",
        "df_temp8['TEAM NO'] = df_temp8['TEAM NO'].astype(str).str.strip()\n",
        "#df_temp9['TEAM NO'] = df_temp9['TEAM NO'].astype(str).str.strip()\n",
        "\n",
        "# Remove duplicates\n",
        "df_all = df_all.drop_duplicates(subset='TEAM NO')\n",
        "df_temp = df_temp.drop_duplicates(subset='TEAM NO')\n",
        "df_temp2 = df_temp2.drop_duplicates(subset='TEAM NO')\n",
        "df_temp3 = df_temp3.drop_duplicates(subset='TEAM NO')\n",
        "df_temp4 = df_temp4.drop_duplicates(subset='TEAM NO')\n",
        "df_temp5 = df_temp5.drop_duplicates(subset='TEAM NO')\n",
        "df_temp6 = df_temp6.drop_duplicates(subset='TEAM NO')\n",
        "df_temp7 = df_temp7.drop_duplicates(subset='TEAM NO')\n",
        "df_temp8 = df_temp8.drop_duplicates(subset='TEAM NO')\n",
        "#df_temp9 = df_temp9.drop_duplicates(subset='TEAM NO')\n",
        "\n",
        "# Set 'TEAM NO' as the index\n",
        "df_all = df_all.set_index('TEAM NO')\n",
        "df_temp = df_temp.set_index('TEAM NO')\n",
        "df_temp2 = df_temp2.set_index('TEAM NO')\n",
        "df_temp3 = df_temp3.set_index('TEAM NO')\n",
        "df_temp4 = df_temp4.set_index('TEAM NO')\n",
        "df_temp5 = df_temp5.set_index(\"TEAM NO\")\n",
        "\n",
        "df_temp6 = df_temp6.set_index('TEAM NO')\n",
        "df_temp7 = df_temp7.set_index(\"TEAM NO\")\n",
        "df_temp8 = df_temp8.set_index('TEAM NO')\n",
        "#df_temp9 = df_temp9.set_index('TEAM NO')\n",
        "\n",
        "# Optional: Rename columns to avoid conflicts\n",
        "df_temp.columns = [f\"ShootingSplits_{col}\" for col in df_temp.columns]\n",
        "df_temp2.columns = [f\"TeamRankings_Away_{col}\" for col in df_temp2.columns]\n",
        "df_temp3.columns = [f\"TeamRankings_Home_{col}\" for col in df_temp3.columns]\n",
        "df_temp4.columns = [f\"TeamRankings_Neutral_{col}\" for col in df_temp4.columns]\n",
        "df_temp5.columns = [f\"KenPom_Barttorvik{col}\" for col in df_temp5.columns]\n",
        "df_temp6.columns = [f\"Barttorvik_Away_{col}\" for col in df_temp6.columns]\n",
        "df_temp7.columns = [f\"Barttorvik_Home_{col}\" for col in df_temp7.columns]\n",
        "df_temp8.columns = [f\"Barttorvik_Neutral_{col}\" for col in df_temp8.columns]\n",
        "#df_temp9.columns = [f\"EvanMiya_{col}\" for col in df_temp9.columns]\n",
        "\n",
        "# Now join on the index (TEAM NO)\n",
        "df_all = df_all.join(df_temp, how='outer')\n",
        "df_all = df_all.join(df_temp2, how='outer')\n",
        "df_all = df_all.join(df_temp3, how='outer')\n",
        "df_all = df_all.join(df_temp4, how='outer')\n",
        "df_all = df_all.join(df_temp5, how='outer')\n",
        "\n",
        "df_all = df_all.join(df_temp6, how='outer')\n",
        "df_all = df_all.join(df_temp7, how='outer')\n",
        "df_all = df_all.join(df_temp8, how='outer')\n",
        "#df_all = df_all.join(df_temp9, how='outer')\n",
        "# Reset index if needed\n",
        "df_all = df_all.dropna(axis=0)\n",
        "df_all = df_all.reset_index()\n",
        "\n",
        "df_all = df_all[df_all['YEAR'] != 2008]\n",
        "df_all = df_all[df_all['YEAR'] != 2009]\n",
        "df_all = df_all.dropna(axis=0)\n",
        "\n",
        "df_all.rename(columns={'Barttorvik_Neutral_TEAM': 'TEAM'}, inplace=True)\n",
        "print(df_all.shape)\n",
        "print(df_all.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correlate_with_win_pct(df, team_stats_df, team_col='team_name', win_pct_col='win_pct', drop_columns=None):\n",
        "    \"\"\"\n",
        "    Computes Pearson correlation between numeric columns in `df` and historical win percentages from `team_stats_df`.\n",
        "    \n",
        "    Parameters:\n",
        "    - df: DataFrame with team features and a team name column.\n",
        "    - team_stats_df: DataFrame with team names and win percentage (string or float).\n",
        "    - team_col: Column name for team names.\n",
        "    - win_pct_col: Column name for win percentage.\n",
        "    - drop_columns: List of column names to exclude from correlation testing.\n",
        "    \n",
        "    Returns:\n",
        "    - DataFrame with:\n",
        "        - variable_name\n",
        "        - test_statistic (Pearson r)\n",
        "        - p-value\n",
        "        - significant_relationship (Bonferroni corrected)\n",
        "    \"\"\"\n",
        "    if drop_columns is None:\n",
        "        drop_columns = []\n",
        "\n",
        "    team_stats_df = team_stats_df.copy()\n",
        "\n",
        "    # Convert percentage strings like \"40.0%\" to float if needed\n",
        "    if team_stats_df[win_pct_col].dtype == 'object':\n",
        "        team_stats_df[win_pct_col] = (\n",
        "            team_stats_df[win_pct_col]\n",
        "            .str.replace('%', '', regex=False)\n",
        "            .astype(float) / 100\n",
        "        )\n",
        "\n",
        "    # Merge win percentage into df\n",
        "    merged_df = df.merge(team_stats_df[[team_col, win_pct_col]], on=team_col, how='left')\n",
        "    merged_df = merged_df.dropna(subset=[win_pct_col])\n",
        "\n",
        "    variable_names = []\n",
        "    list_corr_values = []\n",
        "    list_p_values = []\n",
        "\n",
        "    for col in merged_df.columns:\n",
        "        if col in [team_col, win_pct_col] or col in drop_columns:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(merged_df[col]):\n",
        "            valid = merged_df[[col, win_pct_col]].dropna()\n",
        "            if len(valid) >= 2:\n",
        "                corr, pval = pearsonr(valid[col], valid[win_pct_col])\n",
        "                variable_names.append(col)\n",
        "                list_corr_values.append(corr)\n",
        "                list_p_values.append(pval)\n",
        "\n",
        "    # Create result DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        'variable_name': variable_names,\n",
        "        'test_statistic (Pearson r)': list_corr_values,\n",
        "        'p-value': list_p_values\n",
        "    })\n",
        "\n",
        "    # Bonferroni correction\n",
        "    alpha = 0.05\n",
        "    num_tests = len(variable_names)\n",
        "    result_df['significant_relationship'] = result_df['p-value'] < (alpha / num_tests)\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pearsons_results = correlate_with_win_pct(df_all, all_time_results_df, team_col=\"TEAM\", win_pct_col='WIN%')\n",
        "pearsons_results.to_csv('person_results.csv', index=False)\n",
        "pearsons_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add visualization here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_all = df_all.drop(columns=['TEAM','YEAR', 'CONF', 'TEAM ID'])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "cols_to_scale = [col for col in df_all.columns if col != 'TEAM NO']\n",
        "df_all_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df_all[cols_to_scale]),\n",
        "    columns=cols_to_scale\n",
        ")\n",
        "df_all_scaled['TEAM NO'] = df_all['TEAM NO']\n",
        "print(df_all_scaled.shape)\n",
        "print(df_all_scaled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matchups = []\n",
        "first_team_no = 0\n",
        "for index, row in matchup_df.iterrows():\n",
        "    if index % 2 == 0:\n",
        "        team_1_no = row[\"TEAM NO\"]\n",
        "        team_1_score = row[\"SCORE\"]\n",
        "        team_1_seed = row[\"SEED\"]\n",
        "    else:\n",
        "        team_0_no = row[\"TEAM NO\"]\n",
        "        team_0_score = row[\"SCORE\"]\n",
        "        team_0_seed = row[\"SEED\"]\n",
        "\n",
        "        winner = 1 if team_1_score > team_0_score else 0\n",
        "        \n",
        "        if team_1_seed == team_0_seed:\n",
        "            lower_seed_won = random.randint(0, 1)\n",
        "        else:\n",
        "            team_1_seed_is_lower = 1 if team_1_seed < team_0_seed else 0\n",
        "            lower_seed_won = 1 if winner == team_1_seed_is_lower else 0\n",
        "\n",
        "        matchup = (team_1_no, team_0_no, winner, lower_seed_won)\n",
        "        matchups.append(matchup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create difference vector of matchup stats\n",
        "pred_winners = []\n",
        "true_winners = []\n",
        "difference_vectors = []\n",
        "\n",
        "skipped = 0  # Track how many matchups we skip\n",
        "\n",
        "for matchup in matchups:\n",
        "    team_1_NO = str(matchup[0])  # Make sure they're strings upfront\n",
        "    team_0_NO = str(matchup[1])\n",
        "\n",
        "    # Check if both teams exist in df_all_scaled\n",
        "    team_1_row = df_all_scaled[df_all_scaled[\"TEAM NO\"] == team_1_NO]\n",
        "    team_0_row = df_all_scaled[df_all_scaled[\"TEAM NO\"] == team_0_NO]\n",
        "\n",
        "    if not team_1_row.empty and not team_0_row.empty:\n",
        "        true_winners.append(matchup[2])  # Now append only if valid\n",
        "\n",
        "        # Get stats and remove \"TEAM NO\" column\n",
        "        team_1_stats = team_1_row.drop(columns=[\"TEAM NO\"]).iloc[0].values\n",
        "        team_0_stats = team_0_row.drop(columns=[\"TEAM NO\"]).iloc[0].values\n",
        "\n",
        "        # Create difference vector\n",
        "        difference_vector = team_1_stats - team_0_stats\n",
        "        difference_vectors.append(difference_vector)\n",
        "    else:\n",
        "        skipped += 1\n",
        "\n",
        "print(f\"✅ Finished creating vectors. Skipped {skipped} matchups due to missing teams.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_lower_seeds_won = 0\n",
        "for matchup in matchups:\n",
        "    total_lower_seeds_won += matchup[3]\n",
        "\n",
        "model_results_df = pd.DataFrame({\n",
        "    'Model': ['Baseline'],\n",
        "    'Accuracy': total_lower_seeds_won / len(matchups)\n",
        "})\n",
        "print(f'Accuracy of Base Model: {total_lower_seeds_won / len(matchups)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## All Features Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Do logistic regression with all features\n",
        "#X_train, X_test, y_train, y_test = train_test_split(difference_vectors, true_winners, test_size=0.2, random_state=42)\n",
        "X = pd.DataFrame(difference_vectors, columns=df_all_scaled.drop(columns=[\"TEAM NO\"]).columns)\n",
        "y = np.array(true_winners)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "LR_model = LogisticRegression()\n",
        "LR_model.fit(X_train, y_train)\n",
        "\n",
        "predicted_winners = LR_model.predict(X_test)\n",
        "\n",
        "new_row = {'Model': ['Linear Regression All'], 'Accuracy': accuracy_score(y_test, predicted_winners)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_winners))\n",
        "\n",
        "\n",
        "\n",
        "# def predict_winner_logR(difference_vector, logistic_regression_model):\n",
        "#     winner = logistic_regression_model.predict(difference_vector)\n",
        "#     return \"Team 1 Wins\" if winner == 1 else \"Team 2 Wins\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Greedy Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure X is a DataFrame\n",
        "X = pd.DataFrame(difference_vectors, columns=df_all_scaled.drop(columns=[\"TEAM NO\"]).columns)\n",
        "y = np.array(true_winners)\n",
        "\n",
        "# Train-test split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Greedy logistic regression\n",
        "greedy_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "feature_scores = {}\n",
        "\n",
        "# First, score each feature individually\n",
        "for feature in X_train.columns:\n",
        "    score = cross_val_score(greedy_lr, X_train[[feature]], y_train, cv=10, scoring='accuracy').mean()\n",
        "    feature_scores[feature] = score\n",
        "\n",
        "# Sort features by their single-feature performance\n",
        "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "selected_features = []\n",
        "best_score = 0\n",
        "\n",
        "for feature, _ in sorted_features:\n",
        "    current_features = selected_features + [feature]\n",
        "    score = cross_val_score(greedy_lr, X_train[current_features], y_train, cv=10, scoring='accuracy').mean()\n",
        "    \n",
        "    if score > best_score:\n",
        "        selected_features.append(feature)\n",
        "        best_score = score\n",
        "        print(f\"✅ Added: {feature} | New CV Accuracy: {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"❌ Skipped: {feature} | CV Accuracy would be: {score:.4f}\")\n",
        "\n",
        "\n",
        "greedy_lr.fit(X_train[selected_features], y_train)\n",
        "\n",
        "\n",
        "predicted_winners_glr = greedy_lr.predict(X_test[selected_features])\n",
        "\n",
        "new_row = {'Model': ['Linear Regression Greedy'], 'Accuracy': accuracy_score(y_test, predicted_winners_glr)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_winners_glr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_binary_mlp(X_train, y_train, X_test, y_test, hidden=64, epochs=10):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(hidden, activation='relu'))\n",
        "    model.add(Dense(hidden, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid')) # output layer, add hidden layers before this\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "    model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "    history = model.fit(x=X_train,y=y_train,\n",
        "                        validation_data = (X_test, y_test),\n",
        "                        batch_size=8,epochs=epochs,\n",
        "                        verbose=1)\n",
        "    \n",
        "    model.summary()\n",
        "    predicted_probabilities = model.predict(X_train)\n",
        "    predicted_probabilities = np.rint(predicted_probabilities)\n",
        "    acc = 100. * accuracy_score(y_train, predicted_probabilities)\n",
        "    print(\"Accuracy on train set: {:.2f}%\".format(acc))\n",
        "    predicted_probabilities = model.predict(X_test)\n",
        "    predicted_probabilities = np.rint(predicted_probabilities)\n",
        "    acc = 100. * accuracy_score(y_test, predicted_probabilities)\n",
        "    print(\"Accuracy on test set: {:.2f}%\".format(acc))\n",
        "    print(confusion_matrix(y_test, predicted_probabilities))\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history, test = run_binary_mlp(X_train, y_train, X_test, y_test,400,50)\n",
        "history2, model_dnn = run_binary_mlp(X_train, y_train, X_test, y_test,400,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    \n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    \n",
        "    plt.title('Training vs Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_probabilities = model_dnn.predict(X_test)\n",
        "predicted_probabilities = np.rint(predicted_probabilities)\n",
        "new_row = {'Model': ['DNN'], 'Accuracy': accuracy_score(y_test, predicted_probabilities)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"DNN\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_probabilities))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(max_depth=1000, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "preds_rf = clf.predict(X_test)\n",
        "new_row = {'Model': ['Random Forest'], 'Accuracy': accuracy_score(y_test, preds_rf)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"Random Forest\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=5)\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "preds_knn = neigh.predict(X_test)\n",
        "new_row = {'Model': ['KNN'], 'Accuracy': accuracy_score(y_test, preds_knn)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"KNN\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtc = DecisionTreeClassifier(random_state=0)\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "preds_dtc = dtc.predict(X_test)\n",
        "new_row = {'Model': ['Decision Tree Classifier'], 'Accuracy': accuracy_score(y_test, preds_dtc)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"Decision Tree\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_dtc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_model = svm.SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "preds_svm = svm_model.predict(X_test)\n",
        "new_row = {'Model': ['SVM'], 'Accuracy': accuracy_score(y_test, preds_svm)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"svm\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gaussian NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "gnb_preds = gnb.fit(X_train, y_train).predict(X_test)\n",
        "new_row = {'Model': ['Gaussian NB'], 'Accuracy': accuracy_score(y_test, gnb_preds)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"gaussianNB\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, gnb_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = greedy_lr.predict(X_test[selected_features])\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of greedy logistical regression\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_probabilities = model_dnn.predict(X_test)\n",
        "predicted_labels = np.rint(predicted_probabilities) \n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix for the DNN\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of Random forest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = neigh.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of KNN\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = dtc.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of Decision Tree\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = svm_model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of svm model\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = gnb.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of GaussianNB model\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_reduced = df_all_scaled.copy()\n",
        "\n",
        "df_reduced = df_reduced.drop(columns=['TEAM NO'])\n",
        "\n",
        "svd = TruncatedSVD(n_components=10, random_state=42) \n",
        "X_svd = svd.fit_transform(df_reduced)\n",
        "X_svd.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_10d = pd.DataFrame(X_svd)\n",
        "df_10d[\"TEAM NO\"] = df_all_scaled['TEAM NO']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_winners2 = []\n",
        "true_winners2 = []\n",
        "difference_vectors2 = []\n",
        "\n",
        "skipped = 0  # Track skipped matchups\n",
        "\n",
        "for matchup in matchups:\n",
        "    team_1_NO = str(matchup[0])\n",
        "    team_0_NO = str(matchup[1])\n",
        "\n",
        "    # Look up rows in df_10d\n",
        "    team_1_row = df_10d[df_10d[\"TEAM NO\"] == team_1_NO]\n",
        "    team_0_row = df_10d[df_10d[\"TEAM NO\"] == team_0_NO]\n",
        "\n",
        "    if not team_1_row.empty and not team_0_row.empty:\n",
        "        true_winners2.append(matchup[2])\n",
        "\n",
        "        # Drop TEAM NO and convert to numpy arrays\n",
        "        team_1_vector = team_1_row.drop(columns=[\"TEAM NO\"]).iloc[0].values\n",
        "        team_0_vector = team_0_row.drop(columns=[\"TEAM NO\"]).iloc[0].values\n",
        "\n",
        "        # Compute difference vector\n",
        "        difference_vector = team_1_vector - team_0_vector\n",
        "        difference_vectors2.append(difference_vector)\n",
        "    else:\n",
        "        skipped += 1\n",
        "\n",
        "print(f\"✅ Finished creating reduced difference vectors. Skipped {skipped} matchups.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "X = pd.DataFrame(difference_vectors2, columns=[f\"SVD_{i+1}\" for i in range(10)])\n",
        "y = np.array(true_winners2)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LR_model_dr = LogisticRegression()\n",
        "LR_model_dr.fit(X_train, y_train)\n",
        "\n",
        "predicted_winners = LR_model_dr.predict(X_test)\n",
        "new_row = {'Model': ['Feature Reduced Linear Regression'], 'Accuracy': accuracy_score(y_test, predicted_winners)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_winners))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Greedy Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Greedy logistic regression\n",
        "greedy_lr_dr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "feature_scores = {}\n",
        "\n",
        "# First, score each feature individually\n",
        "for feature in X_train.columns:\n",
        "    score = cross_val_score(greedy_lr_dr, X_train[[feature]], y_train, cv=10, scoring='accuracy').mean()\n",
        "    feature_scores[feature] = score\n",
        "\n",
        "# Sort features by their single-feature performance\n",
        "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "selected_features = []\n",
        "best_score = 0\n",
        "\n",
        "for feature, _ in sorted_features:\n",
        "    current_features = selected_features + [feature]\n",
        "    score = cross_val_score(greedy_lr_dr, X_train[current_features], y_train, cv=10, scoring='accuracy').mean()\n",
        "    \n",
        "    if score > best_score:\n",
        "        selected_features.append(feature)\n",
        "        best_score = score\n",
        "        print(f\"✅ Added: {feature} | New CV Accuracy: {score:.4f}\")\n",
        "    else:\n",
        "        print(f\"❌ Skipped: {feature} | CV Accuracy would be: {score:.4f}\")\n",
        "\n",
        "\n",
        "greedy_lr_dr.fit(X_train[selected_features], y_train)\n",
        "\n",
        "\n",
        "predicted_winners_glr_dr = greedy_lr_dr.predict(X_test[selected_features])\n",
        "new_row = {'Model': ['Feature Reduced Greedy Linear Regression'], 'Accuracy': accuracy_score(y_test, predicted_winners_glr_dr)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_winners_glr_dr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_dr, model_dnn_dr = run_binary_mlp(X_train, y_train, X_test, y_test,400,50)\n",
        "plot_training_history(history_dr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_probabilities = model_dnn_dr.predict(X_test)\n",
        "predicted_probabilities = np.rint(predicted_probabilities)\n",
        "print(\"DNN\")\n",
        "new_row = {'Model': ['Feature Reduced DNN'], 'Accuracy': accuracy_score(y_test, predicted_probabilities)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_probabilities))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest Classifer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(max_depth=1000, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "preds_rf = clf.predict(X_test)\n",
        "new_row = {'Model': ['Feature Reduced Random Forest'], 'Accuracy': accuracy_score(y_test, preds_rf)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"Random Forest\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=5)\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "preds_knn = neigh.predict(X_test)\n",
        "new_row = {'Model': ['Feature Reduced KNN'], 'Accuracy': accuracy_score(y_test, preds_knn)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"KNN\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_knn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtc = DecisionTreeClassifier(random_state=0)\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "preds_dtc = dtc.predict(X_test)\n",
        "new_row = {'Model': ['Feature Reduced Decision Tree Classifier'], 'Accuracy': accuracy_score(y_test, preds_dtc)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"Decision Tree\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_dtc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_model = svm.SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "preds_svm = svm_model.predict(X_test)\n",
        "new_row = {'Model': ['Feature Reduced SVM'], 'Accuracy': accuracy_score(y_test, preds_svm)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"svm\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "\n",
        "gnb_preds = gnb.fit(X_train, y_train).predict(X_test)\n",
        "new_row = {'Model': ['Feature Reduced Gaussian NB'], 'Accuracy': accuracy_score(y_test, gnb_preds)}\n",
        "model_results_df.loc[len(model_results_df)] = new_row\n",
        "print(\"gaussianNB\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, gnb_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels = LR_model_dr.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predicted_labels))\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of logistical regression with dimensionality reduction\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "predicted_labels = greedy_lr_dr.predict(X_test[selected_features])\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of greedy logistical regression with dimensionality reduction\")\n",
        "plt.show()\n",
        "\n",
        "predicted_probabilities = model_dnn_dr.predict(X_test)\n",
        "predicted_labels = np.rint(predicted_probabilities) \n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix for the DNN with dimensionality reduction\")\n",
        "plt.show()\n",
        "\n",
        "predicted_labels = clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of Random forest with dimensionality reduction\")\n",
        "plt.show()\n",
        "\n",
        "predicted_labels = neigh.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of KNN with dimensionality reduction\")\n",
        "plt.show()\n",
        "\n",
        "predicted_labels = dtc.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of Decision Tree with dimensionality reduction\")\n",
        "plt.show()\n",
        "\n",
        "predicted_labels = svm_model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of svm model with dimensionality reduction\")\n",
        "plt.show()\n",
        "\n",
        "predicted_labels = gnb.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix of GaussianNB model with dimensionality reduction\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_results_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
